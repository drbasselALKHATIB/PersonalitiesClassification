import pandas as pd
# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ÙˆØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª 
tweets = pd.read_csv('tweets.csv',encoding = "utf-8")
# Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø¥Ø·Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
tweets.head()

print('Data size:', tweets.shape)

# Ù…ÙƒØªØ¨Ø©  Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ù†ØµÙŠØ© 
import string
# Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ¹Ø§Ø¨ÙŠØ± Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ©
import re
# Ù…ÙƒØªØ¨Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
import nltk
#nltk.download('punkt')
#nltk.download('stopwords')
# Ù…ÙƒØªØ¨Ø©  ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù
from nltk.corpus import stopwords
# Ù…ÙƒØªØ¨Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ­Ø¯Ø§Øª
from nltk.tokenize import word_tokenize
# Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø¬Ø°Ø¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
from snowballstemmer import stemmer
ar_stemmer = stemmer("arabic")

# Ø¯Ø§Ù„Ø© Ø­Ø°Ù Ø§Ù„Ù…Ø­Ø§Ø±Ù ØºÙŠØ± Ø§Ù„Ù„Ø§Ø²Ù…Ø©
def remove_chars(text, del_chars):
    translator = str.maketrans('', '', del_chars)
    return text.translate(translator)
# Ø¯Ø§Ù„Ø© Ø­Ø°Ù Ø§Ù„Ù…Ø­Ø§Ø±Ù Ø§Ù„Ù…ÙƒØ±Ø±Ø©
def remove_repeating_char(text):
    return re.sub(r'(.)\1{2,}', r'\1', text)

# Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª
def clean_tweet(tweet):
    stop_words = stopwords.words('arabic')
    # Ù…Ø­Ø§Ø±Ù Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ©  
    emoj = re.compile("["
            u"\U0001F600-\U0001F64F"  
            u"\U0001F300-\U0001F5FF"  
            u"\U0001F680-\U0001F6FF"  
            u"\U0001F1E0-\U0001F1FF" 
            u"\U00002500-\U00002BEF"  
            u"\U00002702-\U000027B0"
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001f926-\U0001f937"
            u"\U00010000-\U0010ffff"
            u"\u2640-\u2642" 
            u"\u2600-\u2B55"
            u"\u200d"
            u"\u23cf"
            u"\u23e9"
            u"\u231a"
            u"\ufe0f"  
            u"\u3030"
            u"\u2066"
                        "]+", re.UNICODE)
    tweet = str(tweet)
    # Ø­Ø°Ù @ ÙˆÙ…Ø§ ÙŠØªØ¨Ø¹Ù‡Ø§
    tweet = re.sub("@[^\s]+","",tweet) 
    tweet = re.sub("RT","",tweet) 
    # Ø­Ø°Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    tweet = re.sub(r"(?:\@|http?\://|https?\://|www)\S+", "", tweet)
    # Ø­Ø°Ù Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ©
    tweet =  re.sub(emoj, '', tweet)
    # Ø­Ø°Ù ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù
    tweet = ' '.join(word for word in tweet.split() if word not in stop_words) 
    # Ø­Ø°Ù Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª #
    tweet = tweet.replace("#", "").replace("_", " ") 
    # Ø­Ø°Ù Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
    tweet = re.sub(r'[0-9]+', '', tweet)  
    # Ø­Ø°Ù Ø§Ù„Ù…Ø­Ø§Ø±Ù ØºÙŠØ± Ø§Ù„Ù„Ø§Ø²Ù…Ø©
    # Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    arabic_punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€'''
    # Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø§Ù†ÙƒÙ„ÙŠØ²ÙŠØ©
    english_punctuations = string.punctuation
    # Ø¯Ù…Ø¬ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø§Ù†ÙƒÙ„ÙŠØ²ÙŠØ©
    punctuations_list = arabic_punctuations + english_punctuations
    tweet = remove_chars(tweet, punctuations_list)
    # Ø­Ø°Ù Ø§Ù„Ù…Ø­Ø§Ø±Ù Ø§Ù„Ù…ÙƒØ±Ø±Ø©  
    tweet = remove_repeating_char(tweet)
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨ÙØ±Ø§ØºØ§Øª  
    tweet = tweet.replace('\n', ' ')  
    # Ø­Ø°Ù Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† ÙˆØ§Ù„ÙŠØ³Ø§Ø±   
    tweet = tweet.strip(' ')  
    return tweet

# Ø¯Ø§Ù„Ø© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø§Øª
def tokenizingText(text): 
    tokens_list = word_tokenize(text) 
    return tokens_list

# Ø¯Ø§Ù„Ø© Ø­Ø°Ù ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù
def filteringText(tokens_list):  
    # Ù‚Ø§Ø¦Ù…Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    listStopwords = set(stopwords.words('arabic'))
    filtered = []
    for txt in tokens_list:
        if txt not in listStopwords:
            filtered.append(txt)
    tokens_list = filtered 
    return tokens_list

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¬Ø°ÙŠØ¹  
def stemmingText(tokens_list): 
    tokens_list = [ar_stemmer.stemWord(word) for word in tokens_list]
    return tokens_list
    
# Ø¯Ø§Ù„Ø© Ø¯Ù…Ø¬ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø¬Ù…Ù„Ø©
def toSentence(words_list):  
    sentence = ' '.join(word for word in words_list)
    return sentence

# Ù…Ø«Ø§Ù„
text= "Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ÙŠÙ‚Ø© ğŸŒØŒ ÙƒÙ„ ÙŠÙˆÙ… 9 ØµØ¨Ø§Ø­Ø§Ù‹ØŒ Ù…Ø¹ Ø±ÙØ§Ù‚ÙŠ Ù‡Ø¤Ù„Ø§Ø¡! @toto  "
print(text)
text=clean_tweet(text)
print(text)
tokens_list=tokenizingText(text)
print(tokens_list)
tokens_list=filteringText(tokens_list)
print(tokens_list)
tokens_list=stemmingText(tokens_list)
print(tokens_list)

# Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª
def process_tweet(tweet):
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©
    tweet=clean_tweet(tweet)
    # Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª
    tweet=tokenizingText(tweet)
    # Ø§Ù„ØªØ¬Ø°ÙŠØ¹
    tweet=stemmingText(tweet)
    return tweet

# Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù„Ù„ØªØºØ±ÙŠØ¯Ø§Øª 
tweets['tweet'] = tweets['tweet'].apply(process_tweet)

# Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø±Ø³Ù…
import matplotlib.pyplot as plt
import seaborn as sns
# Ø­Ø¬Ù… Ø§Ù„Ø±Ø³Ù…
plt.figure(figsize=(12, 6))
# Ø±Ø³Ù… Ø¹Ø¯Ø¯ ÙƒÙ„ ØµÙ
sns.countplot(data=tweets, y='topic');
plt.title('Topics Distribution', fontsize=18)

plt.show()

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø©
from imblearn.over_sampling import RandomOverSampler
# Ø¥Ù†Ø´Ø§Ø¡ ØºØ±Ø¶ Ù…Ù† Ø§Ù„ØµÙ
oversample = RandomOverSampler()
# ØªÙˆÙ„ÙŠØ¯ Ø³Ø·Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ
tweets = tweets.sample(frac=1)
# ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
tweets, Y = oversample.fit_resample(tweets, tweets.topic)

# Ø¥Ø¹Ø§Ø¯Ø© Ø±Ø³Ù… Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙÙˆÙ
# Ø¨Ø¹Ø¯ Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø©
plt.figure(figsize=(12, 6))
sns.countplot(data=tweets, y='topic');
plt.title('Topics Distribution After OverSampling', fontsize=18)

plt.show()

from sklearn.preprocessing import LabelEncoder
# ØªØ±Ù…ÙŠØ² Ø§Ù„ØµÙÙˆÙ
le_topics = LabelEncoder()
tweets['topic'] = tweets[['topic']].apply(le_topics.fit_transform)

classes = le_topics.classes_ # Ø§Ù„ØµÙÙˆÙ

n_classes = len(classes) # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ
print("No. of classes:", n_classes)
print("Classes:", classes)
print("Coding: ", le_topics.transform(classes))

from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

# Ù†Ø±ÙƒÙŠØ¨ Ø¬Ù…Ù„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ù…Ù† Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
sentences = tweets['tweet'].apply(toSentence)
print(sentences[6]) 
# Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ø¹Ø¸Ù…ÙŠ Ø°Ø§Øª Ø§Ù„ØªÙˆØ§ØªØ± Ø§Ù„Ø£ÙƒØ¨Ø±
# Ø§Ù„ØªÙŠ Ø³ØªÙØ³ØªØ®Ø¯Ù…
max_words = 5000
# Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ø¹Ø¸Ù…ÙŠ Ù„Ø´Ø¹Ø§Ø¹ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
max_len = 50
#   Ø§Ù„ØªØµØ±ÙŠØ­ Ø¹Ù† Ø§Ù„Ù…Ø¬Ø²Ø¦ 
#   Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ Ø³ØªØ¨Ù‚Ù‰ 
#  Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ ØªÙˆØ§ØªØ±Ù‡Ø§ 
tokenizer = Tokenizer(num_words=max_words )
# Ù…Ù„Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¬Ø²Ø¦ Ù„Ù†ØµÙˆØµ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª
print(sentences[0])
tokenizer.fit_on_texts(sentences)
# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
S = tokenizer.texts_to_sequences(sentences)
print(S[0]) 
# ØªÙˆØ­ÙŠØ¯ Ø£Ø·ÙˆØ§Ù„ Ø§Ù„Ø£Ø´Ø¹Ø©
X = pad_sequences(S, maxlen=max_len)
print(X[0]) 
X.shape

# ØªÙˆÙ„ÙŠØ¯ Ø´Ø¹Ø§Ø¹ Ø§Ù„Ø®Ø±Ø¬
y = tweets['topic']

# Ù…ÙƒÙ†Ø¨Ø© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±
from sklearn.model_selection import train_test_split

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
print(X_train[0])
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

# ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠ
from keras.models import Sequential
# ØªØ¶Ù…ÙŠÙ†  Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©
from keras.layers import Embedding, Dense, LSTM
# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
from tensorflow.keras.optimizers import Adam, RMSprop 

# Ø§Ù„ØªØµØ±ÙŠØ­ Ø¹Ù† Ø¯Ø§Ù„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù…
# Ù…Ø¹ Ø¥Ø¹Ø·Ø§Ø¡ Ù‚ÙŠÙ… Ø£ÙˆÙ„ÙŠØ© Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ±ÙØ¹Ø©
def create_model(embed_dim = 32, hidden_unit = 16, dropout_rate = 0.2, 
optimizers = RMSprop, learning_rate = 0.001):
    # Ø§Ù„ØªØµØ±ÙŠØ­ Ø¹Ù† Ù†Ù…ÙˆØ°Ø¬ ØªØ³Ù„Ø³Ù„ÙŠ
    model = Sequential()
    # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
    model.add(Embedding(input_dim = max_words, output_dim = embed_dim, input_length = max_len))
    # LSTM
    model.add(LSTM(units = hidden_unit ,dropout=dropout_rate))
    # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
    model.add(Dense(units = len(classes), activation = 'softmax'))
    # ÙŠÙ†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizers(learning_rate = learning_rate), metrics = ['accuracy'])
    # Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print(model.summary())
 
    return model

# Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØµÙ†ÙŠÙ
from keras.wrappers.scikit_learn import KerasClassifier

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ±ÙØ¹Ø© Ø§Ù„Ø£Ù…Ø«Ù„ÙŠØ©

model = KerasClassifier(build_fn = create_model,
                        # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                        dropout_rate = 0.2,
                        embed_dim = 32,
                        hidden_unit = 64,
                        optimizers = Adam,
                        learning_rate = 0.001,
                        # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
                        epochs=10, 
                        batch_size=256,
                        # Ù†Ø³Ø¨Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
                        validation_split = 0.1)
# Ù…Ù„Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
model_prediction = model.fit(X_train, y_train)

# Ù…Ø¹Ø§ÙŠÙ†Ø© Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
fig, ax = plt.subplots(figsize = (10, 4))
ax.plot(model_prediction.history['accuracy'], label = 'train accuracy')
ax.plot(model_prediction.history['val_accuracy'], label = 'val accuracy')
ax.set_title('Model Accuracy')
ax.set_xlabel('Epoch')
ax.set_ylabel('Accuracy')
ax.legend(loc = 'upper left')
plt.show()

# Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡

# Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„ØµØ­Ø©
from sklearn.metrics import accuracy_score 
# Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¯Ù‚Ø©
from sklearn.metrics import precision_score
# Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø§Ø³ØªØ°ÙƒØ§Ø±
from sklearn.metrics import recall_score
# f1
from sklearn.metrics import f1_score
# Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ
from sklearn.metrics import confusion_matrix
# ØªØµÙ†ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
y_pred = model.predict(X_test)
# Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
accuracy = accuracy_score(y_test, y_pred)
precision=precision_score(y_test, y_pred , average='weighted')
recall= recall_score(y_test, y_pred, zero_division=1, average='weighted')
f1= f1_score(y_test, y_pred, zero_division=1,  average='weighted')

print('Model Accuracy on Test Data: {:.2f}'.format(accuracy*100))
print('Model Precision on Test Data: {:.2f}'.format(precision*100))
print('Model Recall on Test Data: {:.2f}'.format(recall*100))
print('Model F1 on Test Data: {:.2f}'.format(f1*100))

confusion_matrix(y_test, y_pred)


# Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ
import seaborn as sns
sns.set(style = 'whitegrid')

fig, ax = plt.subplots(figsize = (8,6))
sns.heatmap(confusion_matrix(y_true = y_test, y_pred = y_pred), fmt = 'g', annot = True)
ax.xaxis.set_label_position('top')
ax.xaxis.set_ticks_position('top')
ax.set_xlabel('Prediction', fontsize = 14)
ax.set_ylabel('Actual', fontsize = 14)
plt.show()

# Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø­Ø§Ù„Ø© Ø£ÙƒØ«Ø± Ù…Ù† ØµÙÙŠÙ†
print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y_test, y_pred)))

print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))

# ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ
from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test, y_pred, target_names=classes))

# Ø¯Ø§Ù„Ø© ØªØµÙ†ÙŠÙ ØªØºØ±ÙŠØ¯Ø©
def classify_tweet(tweet):
    # ØªØ­ÙˆÙŠÙ„ Ø´Ø¹Ø§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ù‰ Ø¬Ù…Ù„Ø©
    tweet = toSentence(tweet)
    # ÙˆØ¶Ø¹ Ø§Ù„Ø¬Ù…Ù„Ø© ÙÙŠ Ø´Ø¹Ø§Ø¹
    ar=[]
    ar.append(tweet)
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
    seq = tokenizer.texts_to_sequences(ar)
    # ØªÙˆØ­ÙŠØ¯ Ø·ÙˆÙ„ Ø§Ù„Ù…ØªØ¬Ù‡Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
    pseq = pad_sequences(seq, maxlen=max_len)
    # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
    pred = model.predict(pseq)
    return pred

# Ø¯Ø§Ù„Ø© ØªØµÙ†ÙŠÙ Ø§Ù„Ø´Ø®Øµ
def classify_person(person_name):
    # ØªØ­Ù…ÙŠÙ„ ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ø´Ø®Øµ
    # ÙÙŠ Ø¥Ø·Ø§Ø± Ø¨ÙŠØ§Ù†Ø§Øª
    path = person_name + '.csv'
    df = pd.read_csv(path)
    # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ù„Ø¹Ø¯
    # Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ù…Ù† ÙƒÙ„ ØµÙ
    classes_count=dict()
    # Ø¥Ø¹Ø·Ø§Ø¡ Ù‚ÙŠÙ… Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠÙ‡ 0
    for i in range(len(classes)):
        key=classes[i]
        classes_count[key]=0
    # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø·ÙˆÙ„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©
    min_tweet_len=5

    total=0
    for _, row in df.iterrows():
        tweet=row['tweet']
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©
        processed_tweet=process_tweet(tweet)
        if len(processed_tweet)>min_tweet_len:
          # ØªØµÙ†ÙŠÙ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©
          c= classify_tweet(processed_tweet)
          # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ø³Ù… Ø§Ù„ØµÙ Ù…Ù† Ø±Ù…Ø²Ù‡
          topic=le_topics.inverse_transform(c)[0]
          # Ø¥Ø¶Ø§ÙØ© 1 Ù„Ù„ØµÙ Ø§Ù„Ù…ÙˆØ§ÙÙ‚
          classes_count[topic]=classes_count[topic]+1
          total=total+1

    # ØªØ±ØªÙŠØ¨ Ø§Ù„ØµÙÙˆÙ ÙˆÙÙ‚ Ø§Ù„Ø¹Ø¯Ø¯
    # ØªÙ†Ø§Ø²Ù„ÙŠØ§Ù‹

    sorted_classes = sorted(classes_count, key=classes_count.get,reverse=True)  
 
    # Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    sorted_classes_cleaned = {}
    min_display=total/25
    # Ø¥Ù‡Ù…Ø§Ù„ Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ØµØºÙŠØ±
    for w in sorted_classes:
      if classes_count[w]>min_display:
        sorted_classes_cleaned[w] = classes_count[w]

    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print(sorted_classes_cleaned) 
    n=0
    for key, value in sorted_classes_cleaned.items():
      n=n+value

    print(person_name, "is classified as :")
    for key, value in sorted_classes_cleaned.items():  
      print(key, "(", "{:.2f}".format((value/n)*100) , "%)")
        
    # Ø±Ø³Ù… ÙØ·ÙŠØ±Ø© Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙÙˆÙ
    x = sorted_classes_cleaned.keys()
    y = sorted_classes_cleaned.values()

    import matplotlib.pyplot as plt
    # pie
    plt.figure(figsize=(9,9));
    plt.title(person_name, fontdict = {'fontsize':20})
    plt.pie(y, labels = x,autopct='%1.1f%%')
    plt.show()
# Ù…Ø«Ø§Ù„
classify_person("salem")

